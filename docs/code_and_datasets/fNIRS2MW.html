<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>The Tufts fNIRS to Mental Workload Dataset | Tufts HCI Lab</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="Tufts Human-Computer Interaction Laboratory">
    
    <link rel="preload" href="/assets/css/0.styles.8ebe1c04.css" as="style"><link rel="preload" href="/assets/js/app.ae6e9df8.js" as="script"><link rel="preload" href="/assets/js/24.dd08a693.js" as="script"><link rel="prefetch" href="/assets/js/1.17b42134.js"><link rel="prefetch" href="/assets/js/10.56b2f126.js"><link rel="prefetch" href="/assets/js/11.9f3468d0.js"><link rel="prefetch" href="/assets/js/12.56682645.js"><link rel="prefetch" href="/assets/js/13.ff14027f.js"><link rel="prefetch" href="/assets/js/14.33d8f7e1.js"><link rel="prefetch" href="/assets/js/15.c99d01e0.js"><link rel="prefetch" href="/assets/js/16.e59a556f.js"><link rel="prefetch" href="/assets/js/17.aab61c20.js"><link rel="prefetch" href="/assets/js/18.04374e90.js"><link rel="prefetch" href="/assets/js/19.4b5ca61e.js"><link rel="prefetch" href="/assets/js/2.be9f799f.js"><link rel="prefetch" href="/assets/js/20.82f46abf.js"><link rel="prefetch" href="/assets/js/21.e4c450dd.js"><link rel="prefetch" href="/assets/js/22.872ae7cb.js"><link rel="prefetch" href="/assets/js/23.30f55882.js"><link rel="prefetch" href="/assets/js/25.d5fb39a3.js"><link rel="prefetch" href="/assets/js/26.877766cc.js"><link rel="prefetch" href="/assets/js/27.0baaad08.js"><link rel="prefetch" href="/assets/js/28.c8e3ce08.js"><link rel="prefetch" href="/assets/js/29.ff8298e9.js"><link rel="prefetch" href="/assets/js/3.c800d1e7.js"><link rel="prefetch" href="/assets/js/30.2a8dcb83.js"><link rel="prefetch" href="/assets/js/31.a871ccb5.js"><link rel="prefetch" href="/assets/js/32.35ef4441.js"><link rel="prefetch" href="/assets/js/33.22538723.js"><link rel="prefetch" href="/assets/js/34.e62e4103.js"><link rel="prefetch" href="/assets/js/35.1594349a.js"><link rel="prefetch" href="/assets/js/36.362d6ddf.js"><link rel="prefetch" href="/assets/js/37.1b7080d3.js"><link rel="prefetch" href="/assets/js/38.ba204a51.js"><link rel="prefetch" href="/assets/js/39.ad7d55d1.js"><link rel="prefetch" href="/assets/js/4.1c95ae87.js"><link rel="prefetch" href="/assets/js/40.a4cb7ef2.js"><link rel="prefetch" href="/assets/js/41.ea37018c.js"><link rel="prefetch" href="/assets/js/42.e33b3d17.js"><link rel="prefetch" href="/assets/js/43.bcd67260.js"><link rel="prefetch" href="/assets/js/44.ebcc084d.js"><link rel="prefetch" href="/assets/js/45.d4218f0d.js"><link rel="prefetch" href="/assets/js/46.c782dc0b.js"><link rel="prefetch" href="/assets/js/47.01ef44c5.js"><link rel="prefetch" href="/assets/js/48.a02edbf5.js"><link rel="prefetch" href="/assets/js/49.ffe0fb15.js"><link rel="prefetch" href="/assets/js/5.13bf9898.js"><link rel="prefetch" href="/assets/js/50.87814056.js"><link rel="prefetch" href="/assets/js/51.34b9d628.js"><link rel="prefetch" href="/assets/js/52.e4e74c1b.js"><link rel="prefetch" href="/assets/js/7.e1e0b0ac.js"><link rel="prefetch" href="/assets/js/8.33c47f5f.js"><link rel="prefetch" href="/assets/js/9.85d41e35.js">
    <link rel="stylesheet" href="/assets/css/0.styles.8ebe1c04.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><section id="global-layout" data-v-4ba9589d><header class="header" data-v-780415cf data-v-4ba9589d><div class="header-navbar" data-v-780415cf><div class="flex-xbc main header-nav" data-v-780415cf><div class="nav-link" data-v-780415cf><a href="/" class="inblock link-logo router-link-active" data-v-780415cf><img data-src="/logo_hci.png" loading="lazy" alt="logo" class="logo-img lazy" data-v-780415cf></a> <br data-v-780415cf> <nav class="link-list" data-v-780415cf><a href="/" class="list-item router-link-active" data-v-780415cf>Home</a><a href="/hci_at_tufts/" class="list-item" data-v-780415cf>HCI At Tufts</a><a href="/people/" class="list-item" data-v-780415cf>People</a><a href="/projects/" class="list-item" data-v-780415cf>Projects</a><a href="/publications/" class="list-item" data-v-780415cf>Publications</a><a href="/code_and_datasets/" class="list-item router-link-active" data-v-780415cf>Code &amp; Datasets</a><a href="/admissions/" class="list-item" data-v-780415cf>Admissions</a><a href="/hci_resources/" class="list-item" data-v-780415cf>HCI Resources</a></nav></div> <!----></div></div> </header> <!----> <section class="page" data-v-4ba9589d data-v-4ba9589d><section class="info" style="background-image:url(/code_and_datasets/fNIRS2MW/fNIRS.png);" data-v-441751fb><article class="main info-content" data-v-22a41398 data-v-441751fb><div class="content-header" data-v-22a41398><h1 class="header-title" data-v-22a41398>The Tufts fNIRS to Mental Workload Dataset</h1></div> <div class="flex-wcc content-tag" data-v-22a41398><div class="inblock tag-list" data-v-22a41398><a href="/category/null/" class="tag-text" data-v-22a41398>
      </a></div> <span class="tag-space" data-v-22a41398>/</span> <div class="inblock tag-list" data-v-22a41398><a href="/tag/BCI/" class="tag-text" data-v-22a41398>BCI
      </a><a href="/tag/fNIRS/" class="tag-text" data-v-22a41398>fNIRS
      </a><a href="/tag/machine learning/" class="tag-text" data-v-22a41398>machine learning
      </a><a href="/tag/time series classification/" class="tag-text" data-v-22a41398>time series classification
      </a><a href="/tag/cognitive workload/" class="tag-text" data-v-22a41398>cognitive workload
      </a></div></div> <div class="content content__default" data-v-22a41398><p>Welcome to the Tufts fNIRS to Mental Workload (fNIRS2MW) open-access dataset!</p> <p>Using this dataset, we can train and evaluate machine learning classifiers that consume a short window (30 seconds) of multivariate fNIRS recordings and predict the mental workload intensity of the user during that interval.</p> <p>We have collected fNIRS brain activity recordings from 68 partipants during a series of controlled n-back experimental tasks designed to induce working memory workloads of varying relative intensity.</p> <p>Of all available datasets suitable for building mental workload classifiers, this dataset is the largest known to us by a factor of 2.5! Plus, it supports fairness audits by age, gender, and race.</p> <h2 id="quick-links"><a href="#quick-links" class="header-anchor">#</a> Quick Links</h2> <ul><li><a href="https://tufts.box.com/s/1e0831syu1evlmk9zx2pukpl3i32md6r" target="_blank" rel="noopener noreferrer">Dataset files on Box.com<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>Dataset License: <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">CC-By-4.0<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/tufts-ml/fNIRS-mental-workload-classifiers" target="_blank" rel="noopener noreferrer">Code on Github<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <p>Jump to: [<a href="#motivation">Project Motivation</a>] |  [<a href="#summary">Dataset Summary</a>] | [<a href="#publications">Publications</a>] | [<a href="#collection">Data Collection Procedures</a>]</p> <p>If you have any questions, please reach out to Leon (leonwang(at)cs.tufts.edu)</p> <h1 id="project-motivation"><a href="#project-motivation" class="header-anchor">#</a> <a name="motivation"> Project Motivation </a></h1> <p>We are interested in building <em>brain computer interfaces</em> (<strong>BCIs</strong>) that would help out everyday computer users working at a desktop or laptop. In our target future use case, a user would actively use a keyboard and mouse as usual, but also wear a non-intrusive headband sensor that would passively provide real-time measurements of brain activity to the computer. Based on moment-to-moment estimates of mental workload, the computer could adjust the interface to support the user.</p> <p>Functional near-infrared spectroscopy (<strong>fNIRS</strong>) is a promising sensor technology for achieving this goal of &quot;everyday BCI&quot;. We have developed a prototype fNIRS probe mounted on a headband that we used to collect this dataset.</p> <p>Looking at the first decade of research on using fNIRS to estimate mental workload, we have identified three barriers that stand in the way of building effective mental workload classifiers using fNIRS data:</p> <ul><li><ol><li><strong><em>The lack of available data</em></strong>. Previous work typically collects proprietary datasets from only 10-30 subjects, often from very homogenous populations. Much larger, more heterogeneous open-access datasets are needed to build everyday BCI systems that work for many users.</li></ol></li> <li><ol start="2"><li><strong><em>The lack of standardized evaluation protocols</em></strong>. Many previous efforts to build mental workload classifiers do not follow best practices in terms of dividing data into training and test sets in a way that allows reliable assessment of generalization potential (how well will this work for a new user?). For many works, it is not even clear how to reproduce their train/test splits. The release of a dataset should be accompanied by a standardized protocol, so that other teams can follow the very same experimental design, which makes results comparable and enables scientific progress.</li></ol></li> <li><ol start="3"><li><strong><em>The high cross-subject variability in fNIRS data</em></strong>. Collected sensor data varies due to differences in individual physiology, sensor placement, and other sources of noise. It is important that datasets represent this diversity so that progress can be made in generalizing to new users and new sessions.</li></ol></li></ul> <p>Our project's <strong>contributions</strong> are:</p> <ul><li><p>We release <strong><em>a large open-access dataset</em></strong> of 68 participants. This dataset is the largest known to us by a factor of 2.5.</p></li> <li><p>We provide a <strong><em>standardized protocol</em></strong> for evaluating classifiers and report benchmark results on our data under three paradigms of training: subject-specific, generic, and generic +
fine-tuning. See our <a href="https://openreview.net/pdf?id=QzNHE7QHhut" target="_blank" rel="noopener noreferrer">paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and <a href="https://github.com/tufts-ml/fNIRS-mental-workload-classifiers" target="_blank" rel="noopener noreferrer">code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> for details.</p></li> <li><p>We provide <strong><em>rich demographic information for each subject</em></strong> (age, gender, race, handedness) which enables auditing the &quot;fairness&quot; of fNIRS classifier performance across subpopulations. We view such audits as essential to make sure BCI works for everybody.</p></li> <li><p>Our dataset is targeted at <strong><em>everyday BCI</em></strong>. Our headband-mounted sensor is non-intrusive and easy to put on / take off when performing everyday tasks. It does not require a fabric cap covering the whole skull (like many alternatives) and does not require any complicated registration to landmarks.</p></li></ul> <h1 id="dataset-summary"><a href="#dataset-summary" class="header-anchor">#</a> <a name="summary"> Dataset Summary </a></h1> <p>For a complete dataset summary, see our public <a href="https://raw.githubusercontent.com/tufts-ml/fNIRS-mental-workload-classifiers/main/Datasheet-Tufts-fNIRS2MW.pdf" target="_blank" rel="noopener noreferrer">Datasheet PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>For each participant (68 recommended; 87 total), the dataset contains the following records obtained during one 30-60 minute experimental session. Each subject contributes just over 21 minutes of fNIRS data from the desired n-back experimental conditions, with remaining time related to rest or instruction periods.</p> <ul><li><p>fNIRS recordings</p> <ul><li>a multivariate time-series representing brain activity throughout the session, recorded by a sensor probe placed on the forehead and secured via headband</li> <li>All measurements are recorded at a regular sampling rate of 5.2 Hz.</li> <li>At each timestep, we record 8 real-valued measurements, one for each combination of 2 measured concentration changes (oxygenated hemoglobin and deoxygenated hemoglobin), 2 optical data types (intensity and phase), and 2 spatial locations on the forehead. The units of each measurement are (change in) micro-moles of (oxy-/deoxy-)hemoglobin per liter of tissue.</li></ul></li> <li><p>Activity labels</p> <ul><li>Annotations of the experimental task activity the subject performed throughout the session, including instruction, rest, and active experiment segments.</li> <li>We label each segment of the active experiment as one of four possible n-back working memory intensity levels (0-back, 1-back, 2-back, or 3-back). Increased intensity levels are intended to induce an increased level of cognitive workload.</li></ul></li> <li><p>Demographics</p> <ul><li>The participant’s age, gender, race, handedness, and other attributes. This lets us measure and audit performance by subpopulation (e.g. how does the classifier perform on white subjects vs. black subjects).</li></ul></li> <li><p>Activity performance</p> <ul><li>We record the participant’s correctness at the working memory tasks (did they correctly identify when the stimuli was the same as one from n steps ago?). This lets us assess data quality and enforce eligibility criteria.</li></ul></li></ul> <h3 id="sliding-window-fnirs-data-for-classifiers"><a href="#sliding-window-fnirs-data-for-classifiers" class="header-anchor">#</a> Sliding window fNIRS data for classifiers</h3> <p>To improve analysis speed and reproducibility, we also make available a preprocessed version of the data that was used in all our reported experiments. We applied bandpass filtering to remove artifacts, and extracted sliding windows of length 30 seconds and stride 0.6 seconds (3 timesteps).</p> <p>Per-subject CSV files containing the sliding window data using windows with 30-second duration and 0.6-second stride are available here: <a href="https://tufts.app.box.com/s/7l8rz3tpos1il637kmhn57mzacdldyv4/folder/144901642550" target="_blank" rel="noopener noreferrer">https://tufts.app.box.com/s/7l8rz3tpos1il637kmhn57mzacdldyv4/folder/144901642550<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><em>Screenshot of the first few lines of the CSV from one subject's data:</em> <img alt="slide window data" data-src="/code_and_datasets/fNIRS2MW/bpf_slide_window_data.png" loading="lazy" class="lazy"></p> <p>Each row of the CSV file represents the fNIRS sensor measurements at one timestep.</p> <p>There are 2 columns used as <em>keys and labels</em>:</p> <ul><li><code>chunk</code> : indicating the index of the window (the window index increments by one as time goes on)</li> <li><code>label</code> : indicating if the intensity level, either 0/1/2/3</li></ul> <p>All rows with the same <code>chunk</code> id represent the time series for a specific individual window. This window of measurements is the input to our classifier.</p> <p>There are 8 measurement columns, which we use as <em>features</em> for machine learning classifiers:</p> <ul><li>AB_I_O, AB_PHI_O, AB_I_DO, AB_PHI_DO</li> <li>CD_I_O, CD_PHI_O, CD_I_DO, CD_PHI_DO</li></ul> <p>Each column is a separate measurement of a hemoglobin concentration in the blood moving through the user's brain. Measuring the changes in these blood oxygen concentrations over time can be a useful proxy of mental workload intensity. The units of each measurement are μmol/L (micromoles per liter).</p> <p>The feature names here indicate:</p> <ul><li>which sensor location on the forehead was used (AB or CD)</li> <li>which optical measurement type was used (I = intensity, PHI= phase)</li> <li>which blood oxygen concentration was being measured (O = oxygenated hemoglobin; DO = deoxygenated hemoglobin)</li></ul> <p>Note: Our box.com data release also offer other window sizes (2, 5, 10, 20, 30, and 40 seconds) if desired. We focused on the 30-second windows, which we identified as preferred.</p> <h1 id="publications"><a href="#publications" class="header-anchor">#</a> <a name="publications"> Publications </a></h1> <p>The paper describing our dataset and benchmarks can be found here:</p> <blockquote><p><a href="https://openreview.net/pdf?id=QzNHE7QHhut">The Tufts fNIRS Mental Workload Dataset &amp; Benchmark for Brain-Computer Interfaces that Generalize</a> <br>
  Zhe Huang, Liang Wang, Giles Blaney, Christopher Slaughter, Devon McKeon, Ziyu Zhou, Robert Jacob, and <a href="https:www.michaelchughes.com">Michael C. Hughes</a> <br>
  To appear in the Proceedings of <i><a href="">Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks </a></i>, 2021
</p></blockquote> <p>Lead authors ZH &amp; LW contributed equally, as did supervisory authors RJ &amp; MCH.</p> <p>We are pleased to report that the paper describing our dataset and benchmarks was recently accepted for publication at the new <a href="">Track on Datasets and Benchmarks happening at NeurIPS 2021</a>. NeurIPS is a top-tier conference for machine learning research.</p> <p>Please cite our papers if you find these datasets (fNIRS2MW-Visual and fNIRS2MW-Audio) useful:</p> <pre>@inproceedings{wang2021taming,
  title={Taming fNIRS-based BCI input for better calibration and broader use},
  author={Wang, Liang and Huang, Zhe and Zhou, Ziyu and McKeon, Devon and Blaney, Giles and Hughes, Michael C and Jacob, Robert JK},
  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages={179--197},
  year={2021}
}
@inproceedings{huangfNIRS2MW2021,
    title = {The Tufts fNIRS Mental Workload Dataset &amp; Benchmark for Brain-Computer Interfaces that Generalize},
    booktitle = {Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks},
    author = {Huang, Zhe and Wang, Liang and Blaney, Giles and Slaughter, Christopher and McKeon, Devon and Zhou, Ziyu and Jacob, Robert J. K. and Hughes, Michael C.},
    year = {2021},
    url = {https://openreview.net/pdf?id=QzNHE7QHhut},
}
@inproceedings{wang2024empower,
  title={Empower Real-World BCIs with NIRS-X: An Adaptive Learning Framework that Harnesses Unlabeled Brain Signals},
  author={Wang, Liang and Zhang, Jiayan and Liu, Jinyang and McKeon, Devon and Brizan, David Guy and Blaney, Giles and Jacob, Robert JK},
  booktitle={Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--16},
  year={2024}
}
</pre> <h1 id="data-collection-procedures"><a href="#data-collection-procedures" class="header-anchor">#</a> <a name="collection"> Data Collection Procedures </a></h1> <hr> <h3 id="irb-and-covid-19-safety-approval"><a href="#irb-and-covid-19-safety-approval" class="header-anchor">#</a> IRB and COVID-19 Safety Approval</h3> <p>Procedures to collect data were approved by <a href="https://viceprovost.tufts.edu/about-sber-irb" target="_blank" rel="noopener noreferrer">Tufts institution's IRB<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and our deidentified dataset was approved for public release (STUDY00000959). Each participant gave informed written consent, and was compensated $20 US.</p> <p>Because collection occurred during the COVID-19 pandemic in early 2021, we also got approval from the Tufts Integrative Safety Committee (ISC). We followed required sanitation and social distancing practices: experimenters used personal protective equipment and disinfected the fNIRS probe for each subject.</p> <hr> <h4 id="task-description"><a href="#task-description" class="header-anchor">#</a> Task Description</h4> <ul><li>Each participant completed <code>16</code> blocks of n-back trails (<em>0→1→2→3→1→2→3→0→2→3→0→1→3→0→1→2</em>).</li> <li>Each block is composed of:
<ul><li><code>40</code> trails ( display for <code>0.5</code> seconds, hidden for <code>1.5</code> seconds).</li></ul></li></ul> <p><em>Details of task sessions is as below:</em> <img alt="task flowchart" data-src="/code_and_datasets/fNIRS2MW/task-flowchart.png" loading="lazy" class="lazy"></p> <hr> <h5 id="task-introduction"><a href="#task-introduction" class="header-anchor">#</a> Task introduction</h5> <p><strong>N-back video introduction</strong>:</p> <p>Before experiment started, the system played a short introductory video, showing an example of a user completing n-back tasks, with voice-over and caption explanations.</p> <p><em>Sample scripts for N-back task video are as below:</em></p> <blockquote><p>Today you will be completing a series of n-back tasks. You will be presented with many numbers, one after another. Some of these numbers will be targets. A number is a target if it is identical to the one shown n steps previously.</p> <p>Here is an example of a 2-back task. In this case, n = 2. The highlighted 8 is a target number, since the number 2 steps back is also 8.</p> <p>Your job will be to press the left arrow key each time you see one of these target numbers. If the number shown is not a target number, you should press the right arrow key.</p></blockquote> <hr> <blockquote><p>Let’s view an example of a 0-back task.</p> <p>At the beginning of each task, you will hear a few beeps which indicate the start of the task.</p> <p>The current type of n-back task can be found at the top of the task window. In this example, n = 0. For the purposes of this demo only, the stack of numbers already shown is included beneath the task window.</p> <p>When n = 0, every number is a target, since every number is identical to the number 0 steps back. That is, every number is the same as itself. For 0-back, the participant presses the left arrow key after each number appears, and never presses the right arrow key.</p></blockquote> <hr> <blockquote><p>Now let’s view an example of a 1-back task.</p> <p>The participant should press the right arrow key for all non-target numbers. When n = 1, a number is only a target if it is identical to the number 1 step back. Once the participant identifies a target number, they should press the left arrow key instead.</p> <p>For example, this 4 is a target number because the previous number was also 4. The participant should press the left arrow key, and then continue pressing the right arrow key until they see the next target number.</p> <p>Here is another target number. The number 1-step back was also 2, so the participant should press the left arrow key.</p> <p>During today’s experiment, the stack of already shown numbers will not be visible to you. It is your job to remember the numbers that were n-steps back so that you are able to identify the correct target numbers.</p></blockquote> <hr> <blockquote><p>There will be 16 rounds in total. During each round, you will complete an n-bask task, where n is either 0, 1, 2, or 3.</p> <p>Before each task, there will be instructions reminding you how to find the target numbers for the current n. During the task, you will be shown 40 numbers. Each time you see a target number, you should press the left arrow key. For all non-target numbers, you should press the right arrow key.</p> <p>After each task, you will complete a survey. Once you have completed the survey, you will be able to rest for 20 seconds.</p> <p>The experiment will take approximately 40 minutes.</p> <p>After the entire experiment is finished, you will complete a short interview about your experience.</p> <p>Please remain seated and do not talk or adjust the headband while completing the experiment. However, if you do feel uncomfortable at any point, let the operator know and they will stop the experiment.</p></blockquote> <hr> <p><strong>Generel introduction</strong>:
Before each task, the system displayed a graphic depicting how to identify targets for the current n, with voice-over.</p> <p><em>The graphic and sample scripts for 0-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_0_back.png" width="50%" height="30%"> <blockquote><p>This is a 0 back task. Every number is a target number.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 1-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_1_back.png" width="50%" height="30%"> <blockquote><p>This is a 1 back task. A number is a target if it is identical to the previous number.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 2-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_2_back.png" width="50%" height="30%"> <blockquote><p>This is a 2 back task. A number is a target if it is identical to the number 2 steps back.</p></blockquote> <hr> <p><em>The graphic and sample scripts for 3-back are as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/intro_3_back.png" width="50%" height="30%"> <blockquote><p>This is a 3 back task. A number is a target if it is identical to the number 3 steps back.</p></blockquote> <hr> <h3 id="data-structure"><a href="#data-structure" class="header-anchor">#</a> Data Structure</h3> <p>Our released dataset includes (<a href="https://tufts.box.com/s/1e0831syu1evlmk9zx2pukpl3i32md6r" target="_blank" rel="noopener noreferrer">Link to fNIRS2MW dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>):</p> <ul><li><strong>fNIRS measurements</strong> in <a href="https://tufts.box.com/s/9jwy67dw2b6emmslutxhmgihk92ivnf2" target="_blank" rel="noopener noreferrer">fNIRS_data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li><strong>Supplementary data</strong>:
<ul><li><strong>demographic and contextual information</strong> in <a href="https://tufts.box.com/s/x2o6hny7kwudg58e1364gdogbvhnwq4f" target="_blank" rel="noopener noreferrer">pre-experiment<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li><strong>Cognitive task performance</strong> in <a href="https://tufts.box.com/s/gme0gsc6z9vj0v4dhchtqh04b4fpy8tm" target="_blank" rel="noopener noreferrer">task_accuracy<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li><strong>experiment log</strong> in <a href="https://tufts.box.com/s/9ruep9b6tpeymda9qrubq64oampfh9br" target="_blank" rel="noopener noreferrer">log<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li><strong>post-experiment interview</strong> in <a href="https://tufts.box.com/s/jrn9rvph8xugstuz9ixra1l2mpfqopdh" target="_blank" rel="noopener noreferrer">interview<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li> <li><strong>subjective workload</strong> in <a href="https://tufts.box.com/s/c5y3m59wwwem0j792epjxhnq4uei5swq" target="_blank" rel="noopener noreferrer">nasa-tlx<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>;</li></ul></li></ul> <div class="language- extra-class"><pre class="language-text"><code>***************************************
** fNIRS2MW dataset folder structure **
***************************************
|- qualified_subjects_list.pdf            
|- pre-experiment                              //
|- experiment                                  //
| |- log                                       //
| |- task_accuracy                             //
| |- fNIRS_data                                //
| | |- raw_data                                //
| | |- band_pass_filtered                      //
| | | |- whole_data                            //
| | | |- slide_window_data                     //
| | | | |- size_02sec_10ts_stride_03ts
| | | | |- size_05sec_25ts_stride_03ts
| | | | |- size_10sec_50ts_stride_03ts
| | | | |- size_20sec_100ts_stride_03ts
| | | | |- size_30sec_150ts_stride_03ts
| | | | |- size_40sec_200ts_stride_03ts
|- post-experiment                             //
| |- nasa-tlx                                  //
| |- interview                                 //
| |- * (all other folders)  
</code></pre></div><hr> <h3 id="data-format"><a href="#data-format" class="header-anchor">#</a> Data Format</h3> <p>We introduce and describe the data format of fNIRS data (raw and pre-processed) and supplementary data as below:</p> <hr> <h4 id="fnirs-data"><a href="#fnirs-data" class="header-anchor">#</a> <a href="https://tufts.box.com/s/9jwy67dw2b6emmslutxhmgihk92ivnf2" target="_blank" rel="noopener noreferrer">fNIRS Data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h4> <p><code>68</code> participants were recruited, aged <code>18</code> to <code>44</code> years. <code>None</code> of the participants reported neurological, psychiatric, or other brain-related diseases that might affect the result.</p> <hr> <h5 id="raw-data"><a href="#raw-data" class="header-anchor">#</a> <a href="https://tufts.box.com/s/i3cphal98hbj3vixj8ypatihwfyer1ot" target="_blank" rel="noopener noreferrer">raw data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>TODO</p> <hr> <h5 id="band-pass-filtered-data"><a href="#band-pass-filtered-data" class="header-anchor">#</a> <a href="https://tufts.box.com/s/bj0iml74am3wnuittzmjggjemxxgz2cp" target="_blank" rel="noopener noreferrer">band pass filtered data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>After pre-processing (<strong>Dual-slope</strong> and <strong>band pass filter</strong>), we have <strong><em>features</em></strong>/<strong><em>columns</em></strong> as below:</p> <ul><li><p>label: The label for each row.</p> <ul><li>The values shoule be in the set {0, 1, 2, 3}, representing 0-back/1-back/2-back/3-back tasks respectively.</li> <li>It should be the same for all rows in the same chunk.</li></ul></li> <li><p>chunk (<strong>Only in slide window data</strong>): The chunk number for each chunk.</p> <ul><li>It starts at <code>0</code> and increases by <code>​1</code> for each chunk.</li> <li>It should be the same for all rows in the same chunk.</li></ul></li> <li><p>AB_I_O, AB_I_DO: Intensity of oxy &amp; deoxy from detector AB.</p></li> <li><p>CD_I_O, CD_I_DO: Intensity of oxy &amp; deoxy from detector CD.</p></li> <li><p>AB_PHI_O, AB_PHI_DO: Phase of oxy &amp; deoxy from detector AB.</p></li> <li><p>CD_PHI_O, CD_PHI_DO: Phase of oxy &amp; deoxy from detector CD.</p></li></ul> <p><img alt="pre process" data-src="/code_and_datasets/fNIRS2MW/pre-processing.png" loading="lazy" class="lazy"></p> <h6 id="whole-data"><a href="#whole-data" class="header-anchor">#</a> <a href="https://tufts.box.com/s/wyvz6utfrd5y5p2js0kj1990dspbnwdn" target="_blank" rel="noopener noreferrer">whole data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h6> <p>Each subject's .csv file includes continuous data of <code>16</code> tasks (exclude data during self-evaluation (<a href="">nasa-tlx</a>) and rest period).</p> <p><em>Screenshot of deidentified data sample is as below:</em> <img alt="whole data" data-src="/code_and_datasets/fNIRS2MW/bpf_whole_data.png" loading="lazy" class="lazy"></p> <hr> <h6 id="slide-window-data"><a href="#slide-window-data" class="header-anchor">#</a> <a href="https://tufts.box.com/s/7l8rz3tpos1il637kmhn57mzacdldyv4" target="_blank" rel="noopener noreferrer">slide window data<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h6> <p>We offer pre processed data in :</p> <ul><li>Window size: <code>10</code>/<code>25</code>/<code>50</code>/<code>100</code>/<code>150</code>/<code>200</code> timestamps (<code>2</code>/<code>5</code>/<code>10</code>/<code>20</code>/<code>30</code>/<code>40</code> seconds)</li> <li>Window stride: <code>3</code> timestamps (rough <code>0.6</code> second)</li></ul> <p><em>Screenshot of deidentified data sample is as below:</em> <img alt="slide window data" data-src="/code_and_datasets/fNIRS2MW/bpf_slide_window_data.png" loading="lazy" class="lazy"></p> <hr> <h4 id="supplementary-data"><a href="#supplementary-data" class="header-anchor">#</a> Supplementary Data</h4> <p>To ensure quality and consistency, we used several criteria to identify which subjects' data are suitable for classifier evaluation.</p> <hr> <h5 id="demographic-and-contextual-information"><a href="#demographic-and-contextual-information" class="header-anchor">#</a> <a href="https://tufts.box.com/s/x2o6hny7kwudg58e1364gdogbvhnwq4f" target="_blank" rel="noopener noreferrer">demographic and contextual information<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>Demographic and contextual information is recorded before the experiment.</p> <p><em>Please see details in the data directly</em></p> <hr> <h5 id="task-accuracy"><a href="#task-accuracy" class="header-anchor">#</a> <a href="https://tufts.box.com/s/gme0gsc6z9vj0v4dhchtqh04b4fpy8tm" target="_blank" rel="noopener noreferrer">task_accuracy<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>We measured the subject's performance at the n-back task based on the accuracy of the subject's response for each digit.</p> <p>the accuracy of each n-back task was recorded.</p> <p><em>Please see details in the screenshot of fake data sample as below:</em></p> <img src="/code_and_datasets/fNIRS2MW/task_accuracy.png" width="30%" height="30%"> <hr> <h5 id="nasa-tlx"><a href="#nasa-tlx" class="header-anchor">#</a> <a href="https://tufts.box.com/s/c5y3m59wwwem0j792epjxhnq4uei5swq" target="_blank" rel="noopener noreferrer">nasa-tlx<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>This is a good way to evaluate the <strong>perceived/subjective</strong> mental workload.</p> <p>Total <code>16</code> &quot;serial_feedback&quot; .csv files for <code>16</code> n-back tasks:</p> <ul><li><code>12</code> files start with &quot;train_1_1_&quot; (<code>1</code>-<code>12</code>) match the first <code>1</code>-<code>12</code> tasks,</li> <li><code>4</code> files start with &quot;test_1_2_&quot; (<code>1</code>-<code>4</code>) match the last <code>4</code> tasks.</li> <li>Useful features include:
<ul><li>movement: subject report if the headband is moved or not,</li> <li>uncomfortable: subject report if feeling unfortable during the experiment or not,</li> <li>mental: mental workload after the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>performance: performance after the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>effort: effort needed for the task from low (<code>0</code>) to high (<code>100</code>)</li> <li>frustration: frustration felt during the task from low (<code>0</code>) to high (<code>100</code>)</li></ul></li></ul> <p><em>Please see details in the screenshot of deidentified data sample as below:</em> <img alt="nasa-tlx" data-src="/code_and_datasets/fNIRS2MW/nasa-tlx.png" loading="lazy" class="lazy"></p> <hr> <h5 id="log"><a href="#log" class="header-anchor">#</a> <a href="https://tufts.box.com/s/9ruep9b6tpeymda9qrubq64oampfh9br" target="_blank" rel="noopener noreferrer">log<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>Hair blocking, light leaking, fNIRS instrument settings and other issues during the experiment were recorded.</p> <hr> <h5 id="interview"><a href="#interview" class="header-anchor">#</a> <a href="https://tufts.box.com/s/jrn9rvph8xugstuz9ixra1l2mpfqopdh" target="_blank" rel="noopener noreferrer">interview<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></h5> <p>Post-experiment interviews were converted from audio to text (pdf version) by the operator.</p> <p>The original audios were <strong>destroyed</strong> immediately following the IRB protocol.</p></div> <div class="content-time" data-v-22a41398><time datetime="Nov 9, 2021" class="time-text" data-v-22a41398>Create Time: Nov 9, 2021
    </time> <time datetime="Nov 11, 2021" class="time-text" data-v-22a41398>Last Updated: Nov 11, 2021
    </time></div></article> <section class="flex-xb main info-nav" data-v-64012905 data-v-441751fb><a href="/code_and_datasets/NIRS-X.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/code_and_datasets/NIRS-X/NIRS-X.jpg" loading="lazy" alt="NIRS-X &amp; fNIRS2MW Audio n-back dataset" class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>NIRS-X &amp; fNIRS2MW Audio n-back dataset</h2> <div class="content" data-v-64012905></div></article></a> <a href="/code_and_datasets/UIST2021.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/code_and_datasets/three_phases.png" loading="lazy" alt="Taming fNIRS-based BCI Input for Better Calibration and Broader Use" class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>Taming fNIRS-based BCI Input for Better Calibration and Broader Use</h2> <div class="content" data-v-64012905></div></article></a></section> <!----></section></section> <div data-v-55aa431a data-v-4ba9589d><div class="my-box" data-v-55aa431a></div> <footer class="footer" data-v-55aa431a><nav class="link-list" data-v-55aa431a><a href="/code_and_datasets/fNIRS2MW.html" aria-current="page" class="list-item router-link-exact-active router-link-active" data-v-55aa431a>HCI Lab, Department of Computer Science 196 Boston Ave., Tufts University, Medford, MA, 02155</a></nav> <a href="/" class="copyright router-link-active" data-v-55aa431a>Copyright  ©  Tufts University School of Engineering. All Rights Reserved.
      </a></footer></div></section><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.ae6e9df8.js" defer></script><script src="/assets/js/24.dd08a693.js" defer></script>
  </body>
</html>
