<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Brain-Computer Interaction using functional Near-Infrared Spectroscopy (fNIRS) | Tufts HCI Lab</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="Tufts Human-Computer Interaction Laboratory">
    
    <link rel="preload" href="/assets/css/0.styles.8ebe1c04.css" as="style"><link rel="preload" href="/assets/js/app.ae6e9df8.js" as="script"><link rel="preload" href="/assets/js/49.ffe0fb15.js" as="script"><link rel="prefetch" href="/assets/js/1.17b42134.js"><link rel="prefetch" href="/assets/js/10.56b2f126.js"><link rel="prefetch" href="/assets/js/11.9f3468d0.js"><link rel="prefetch" href="/assets/js/12.56682645.js"><link rel="prefetch" href="/assets/js/13.ff14027f.js"><link rel="prefetch" href="/assets/js/14.33d8f7e1.js"><link rel="prefetch" href="/assets/js/15.c99d01e0.js"><link rel="prefetch" href="/assets/js/16.e59a556f.js"><link rel="prefetch" href="/assets/js/17.aab61c20.js"><link rel="prefetch" href="/assets/js/18.04374e90.js"><link rel="prefetch" href="/assets/js/19.4b5ca61e.js"><link rel="prefetch" href="/assets/js/2.be9f799f.js"><link rel="prefetch" href="/assets/js/20.82f46abf.js"><link rel="prefetch" href="/assets/js/21.e4c450dd.js"><link rel="prefetch" href="/assets/js/22.872ae7cb.js"><link rel="prefetch" href="/assets/js/23.30f55882.js"><link rel="prefetch" href="/assets/js/24.dd08a693.js"><link rel="prefetch" href="/assets/js/25.d5fb39a3.js"><link rel="prefetch" href="/assets/js/26.877766cc.js"><link rel="prefetch" href="/assets/js/27.0baaad08.js"><link rel="prefetch" href="/assets/js/28.c8e3ce08.js"><link rel="prefetch" href="/assets/js/29.ff8298e9.js"><link rel="prefetch" href="/assets/js/3.c800d1e7.js"><link rel="prefetch" href="/assets/js/30.2a8dcb83.js"><link rel="prefetch" href="/assets/js/31.a871ccb5.js"><link rel="prefetch" href="/assets/js/32.35ef4441.js"><link rel="prefetch" href="/assets/js/33.22538723.js"><link rel="prefetch" href="/assets/js/34.e62e4103.js"><link rel="prefetch" href="/assets/js/35.1594349a.js"><link rel="prefetch" href="/assets/js/36.362d6ddf.js"><link rel="prefetch" href="/assets/js/37.1b7080d3.js"><link rel="prefetch" href="/assets/js/38.ba204a51.js"><link rel="prefetch" href="/assets/js/39.ad7d55d1.js"><link rel="prefetch" href="/assets/js/4.1c95ae87.js"><link rel="prefetch" href="/assets/js/40.a4cb7ef2.js"><link rel="prefetch" href="/assets/js/41.ea37018c.js"><link rel="prefetch" href="/assets/js/42.e33b3d17.js"><link rel="prefetch" href="/assets/js/43.bcd67260.js"><link rel="prefetch" href="/assets/js/44.ebcc084d.js"><link rel="prefetch" href="/assets/js/45.d4218f0d.js"><link rel="prefetch" href="/assets/js/46.c782dc0b.js"><link rel="prefetch" href="/assets/js/47.01ef44c5.js"><link rel="prefetch" href="/assets/js/48.a02edbf5.js"><link rel="prefetch" href="/assets/js/5.13bf9898.js"><link rel="prefetch" href="/assets/js/50.87814056.js"><link rel="prefetch" href="/assets/js/51.34b9d628.js"><link rel="prefetch" href="/assets/js/52.e4e74c1b.js"><link rel="prefetch" href="/assets/js/7.e1e0b0ac.js"><link rel="prefetch" href="/assets/js/8.33c47f5f.js"><link rel="prefetch" href="/assets/js/9.85d41e35.js">
    <link rel="stylesheet" href="/assets/css/0.styles.8ebe1c04.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><section id="global-layout" data-v-4ba9589d><header class="header" data-v-780415cf data-v-4ba9589d><div class="header-navbar" data-v-780415cf><div class="flex-xbc main header-nav" data-v-780415cf><div class="nav-link" data-v-780415cf><a href="/" class="inblock link-logo router-link-active" data-v-780415cf><img data-src="/logo_hci.png" loading="lazy" alt="logo" class="logo-img lazy" data-v-780415cf></a> <br data-v-780415cf> <nav class="link-list" data-v-780415cf><a href="/" class="list-item router-link-active" data-v-780415cf>Home</a><a href="/hci_at_tufts/" class="list-item" data-v-780415cf>HCI At Tufts</a><a href="/people/" class="list-item" data-v-780415cf>People</a><a href="/projects/" class="list-item router-link-active" data-v-780415cf>Projects</a><a href="/publications/" class="list-item" data-v-780415cf>Publications</a><a href="/code_and_datasets/" class="list-item" data-v-780415cf>Code &amp; Datasets</a><a href="/admissions/" class="list-item" data-v-780415cf>Admissions</a><a href="/hci_resources/" class="list-item" data-v-780415cf>HCI Resources</a></nav></div> <!----></div></div> </header> <!----> <section class="page" data-v-4ba9589d data-v-4ba9589d><section class="info" style="background-image:url(/project_images/bci_using_fnirs_audrey/sides.jpg);" data-v-441751fb><article class="main info-content" data-v-22a41398 data-v-441751fb><div class="content-header" data-v-22a41398><h1 class="header-title" data-v-22a41398>Brain-Computer Interaction using functional Near-Infrared Spectroscopy (fNIRS)</h1></div> <div class="flex-wcc content-tag" data-v-22a41398><div class="inblock tag-list" data-v-22a41398><a href="/category/Past Project/" class="tag-text" data-v-22a41398>Past Project
      </a></div> <span class="tag-space" data-v-22a41398>/</span> <div class="inblock tag-list" data-v-22a41398><a href="/tag/null/" class="tag-text" data-v-22a41398>
      </a></div></div> <div class="content content__default" data-v-22a41398><h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <hr> <p>This project brings together three research areas in a combination that is now poised to advance the field of human-computer interaction. We combine new technology for brain activity measurement using functional near infrared spectroscopy (fNIRs), the use of machine learning for analyzing user data in human-computer interaction (HCI), and experience in designing, implementing, and evaluating non-command, adaptive user interfaces from our work on eye movement-based interaction.</p> <h2 id="abstract"><a href="#abstract" class="header-anchor">#</a> Abstract</h2> <hr> <p>In particular, fNIRs is still a research modality. It has rarely been used in combination with either machine learning or HCI, and, to our knowledge, the use of fNIRs as a real-time input to an adaptable interface breaks new ground. We believe this combination will also lead to new, more objective methods for evaluating next generation interaction styles. We will bring our concept of reality-based interaction, described below, to this study. It focuses on the ways that new interaction styles exploit the user's pre-existing skills and expectations from the real world more than trained computer skills. It also helps us differentiate mental effort devoted to interface-related or syntactic aspects from that devoted to the underlying task or semantic aspects. Bringing these three fields together will open up a new area of HCI research. We hope to advance the theory and evaluation of interaction styles as well as the development of new types of interactive interfaces by using human brain activity as 1) a more objective measure for evaluating emerging interaction styles and 2) an input to adaptable user interfaces.<img src="/project_images/bci_using_fnirs_audrey/sides.jpg" style="float:right;"></p> <p>The expertise and collaboration needed to connect fNIRs, machine learning, and HCI is difficult to obtain in a single research team. We are well positioned at Tufts, with strong researchers in all three fields in the same or neighboring departments within the Engineering School. This proposal begins with background information and a review of literature relevant to our proposed research. Next, we present results from our feasibility study, demonstrating that our technical approach is working and our goals appear viable. Finally, we describe four phases of our proposed research and summarize the intellectual merit and broader impacts of the project.</p> <p>The feasibility study results presented below show that, using our fNIRs measurements and machine learning algorithms, we were able to classify five different types of user workload in a computer task with average accuracy exceeding 95%.</p> <p>In Phase 1 of our proposed work, we will refine our process and algorithms to give us a reliable and repeatable procedure for real time user workload measurement. We will also extend our work to continuously varying workload levels and address the removal of motion artifacts from the data.</p> <p>In Phase 2, we will combine EEG measurements with fNIRs, to use their complementary properties to obtain a fuller picture of the user's brain activity.<img src="/project_images/bci_using_fnirs_audrey/brainaudrey.jpg" style="float:right;"></p> <p>Phase 3 applies this technology as a tool to evaluate user interfaces. In particular, we will focus on next generation, non-WIMP, reality-based user interfaces, which are less amenable to traditional evaluation techniques. The need for new techniques to evaluate these interfaces is a growing problem within the field of HCI. The goal of new interfaces might differ from traditional ones. For example, a next generation interface might have an enhanced entertainment value, yet most evaluation tools will measure performance as error rate or time spent on a task. Metrics such as user frustration, workload, and enjoyment have a limited use in evaluation studies because of their subjective nature. We will explore using fNIRs data as an objective measurement of these properties and apply them to the evaluation of non-WIMP interfaces.</p> <p>Phase 4 focuses on creating new interactive, real-time user interfaces, which can adapt their behavior based on the brain measurement information we obtain. The design challenge will be to use this information in a subtle and judicious way, as an additional, lightweight input that could make a mouse or keyboard-driven interface more intuitive or efficient. We will draw on our prior work in designing eye movement-based interaction techniques, because of the strong parallels between the two HCI design problems.</p> <p>If successful, our work will pave the way for a new and useful combination of fNIRs, HCI, and machine learning. We will use it to produce better ways to study, characterize, and measure user interfaces, and we will classify them within the theoretical framework of reality-based interaction. We will then produce new types of interfaces that can adapt to the user's workload profile or other brain activity in real time, and we will also develop technical improvements in applying fNIRs to realistic HCI settings and combining it with other sensors.</p> <h2 id="news-articles"><a href="#news-articles" class="header-anchor">#</a> News Articles</h2> <hr> <ul><li>&quot;For Tired Computer Users: A Headband To Tell You When To Quit,&quot; Infomation Week (October 12, 2007). <a href="http://www.informationweek.com/industries/showArticle.jhtml?articleID=202401723" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;The Computer That Reads Your Mood,&quot; Switched (October 10, 2007). <a href="http://www.switched.com/2007/10/10/the-computer-that-reads-your-mood/" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Tufts Researchers Try to 'Read' Users' Minds,&quot; PC World (October 10, 2007). <a href="http://www.pcworld.com/article/id,138268-c,researchreports/article.html" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Breakthroughs, tips and trends: Mood monitors,&quot; Times Online (October 6, 2007). <a href="http://women.timesonline.co.uk/tol/life_and_style/women/body_and_soul/article2595387.ece" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;'Mind reading' technology aims to help control computers,&quot; EE Times (October 5, 2007). <a href="http://www.eetimes.com/news/latest/showArticle.jhtml;jsessionid=E1PLXMX4NMTT2QSNDLSCKHA?articleID=202201609" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Soon, PCs that can read your mind,&quot; The Times of India (October 4, 2007). <a href="http://timesofindia.indiatimes.com/HealthScience/Soon_PCs_that_can_read_your_mind/articleshow/2426602.cms" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Computer Can Tell How Hard You're Working,&quot; Fox News (October 3, 2007). <a href="http://www.foxnews.com/story/0,2933,299175,00.html" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Mind-Reading Computers,&quot; BBC Focus (October 3, 2007). <a href="http://www.focusmag.co.uk/newsread.asp?ID=34649" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Next gen. computers to mind read,&quot; Press TV (October 3, 2007). <a href="http://www.presstv.ir/detail.aspx?id=25669&amp;sectionid=3510208" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Boffins teach computer to read your mind,&quot; The Inquirer (October 2, 2007). <a href="http://www.theinquirer.net/gb/inquirer/news/2007/10/02/computer-read-mind" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>&quot;Building a computer that reads minds,&quot; MSNBC (October 2, 2007). <a href="http://www.msnbc.msn.com/id/21097949/" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
&quot;Technology Could Enable Computers To 'Read The Minds' Of Users,&quot; Science Daily (October 2, 2007). <a href="http://www.sciencedaily.com/releases/2007/10/071001125649.htm" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="papers"><a href="#papers" class="header-anchor">#</a> Papers</h2> <hr> <h3 id="journal-articles"><a href="#journal-articles" class="header-anchor">#</a> Journal Articles</h3> <ul><li><strong>Discrimination of Mental Workload Levels in Human Subjects with Functional Near-Infrared Spectroscopy</strong>, A. Sassaroli, F. Zheng, L.M. Hirshfield, A. Girouard, E.T. Solovey, R.J.K. Jacob, and S. Fantini, <em>Journal of Innovative Optical Health Sciences</em>, vol. 1, no. 2, pp. 227-237 (2008).</li></ul> <h3 id="refereed-conference-proceedings"><a href="#refereed-conference-proceedings" class="header-anchor">#</a> Refereed Conference Proceedings</h3> <ul><li><p><strong>Distinguishing Difficulty Levels with Non-invasive Brain Activity Measurements</strong>, Girouard, A., Solovey, E.T., Hirshfield, L.M., Chauncey, K., Sassaroli, A., Fantini, S. and Jacob, R.J.K., in <em>INTERACT</em> 2009, (Uppsala, Sweden, 2009), Springer.</p></li> <li><p><strong>Adaptive Brain Computer Interfaces</strong>, Girouard, A., in <em>CHI '09 extended abstracts on Human factors in computing systems</em>, (2009). [link](file:///Users/ziyuzhou/Documents/study/codebase/github_page/hci_lab_website/materials/public_html/papers/dc.girouard.chi09.pdf)</p></li> <li><p><strong>Brain Measurement for Usability Testing and Adaptive Interfaces: An Example of Uncovering Syntactic Workload with Functional Near Infrared Spectroscopy,</strong>, L.M. Hirshfield, E.T. Solovey, A. Girouard, J. Kebinger, R.J.K. Jacob, A. Sassaroli, and S. Fantini, <em>Proc. ACM CHI 2009 Human Factors in Computing Systems Conference, ACM Press</em> (2009). <a href="http://www.cs.tufts.edu/~jacob/papers/chi09.hirshfield.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><strong>Combining Electroencephalograph and Near Infrared Spectroscopy to Explore Users' Instantaneous and Continuous Mental Workload States</strong>, L. Hirshfield, K. Chauncey, E. Solovey, A. Girouard, R. Jacob, A. Sassaroli, and S. Fantini, <em>HCI International 2009 13th International Conference on Human-Computer Interaction</em>, Springer (2009).</p></li></ul> <h3 id="other-published-proceedings"><a href="#other-published-proceedings" class="header-anchor">#</a> Other Published Proceedings</h3> <ul><li><strong>Human-Computer Interaction and Brain Measurement Using Functional Near-Infrared Spectroscopy</strong>, L.M. Hirshfield, A. Girouard, E.T. Solovey, R.J.K. Jacob, A. Sassaroli, Y. Tong, and S. Fantini, <em>ACM UIST 2007 Symposium on User Interface Software and Technology</em>, ACM Press, Poster paper (2007). <a href="http://www.cs.tufts.edu/~jacob/papers/uist07.poster.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h3 id="other-publications"><a href="#other-publications" class="header-anchor">#</a> Other Publications</h3> <ul><li><p><strong>Programming Reality within the Reality-Based Interaction Framework</strong>, E.T. Solovey, O. Shaer, A. Girouard, L.M. Hirshfield, M.S. Horn, J. Zigelbaum, and R.J.K. Jacob, <em>Proc. ACM CHI 2009 Workshop on Programming Reality: From Transitive Materials to Organic User Interfaces</em> (2009). <a href="http://www.cs.tufts.edu/~jacob/papers/organicworkshop.chi09.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><strong>Using Brain Measurement to Evaluate Reality Based Interactions</strong>, L.M. Hirshfield, E.T. Solovey, A. Girouard, R.J.K. Jacob, J. Kebinger, M.S. Horn, O. Shaer, J. Zigelbaum, and R.J.K. Jacob, <em>Proc. ACM CHI 2009 Workshop on Challenges in Evaluating Usability and User Experience of Reality-Based Interaction</em> (2009). <a href="http://www.cs.tufts.edu/~jacob/papers/christouworkshop.chi09.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><strong>Real-time assessment of mental workload with near infrared spectroscopy: potential for human-computer interaction</strong>， A. Sasaroli, Y. Tong, L.M. Hirshfield, A. Girouard, E.T. Solovey, R.J.K. Jacob, and S. Fantini, <em>BIOMED OSA Topical Meeting, Poster presentation</em> (2008). <a href="http://www.cs.tufts.edu/~jacob/papers/poster.osa.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><strong>Using functional Near-Infrared Spectroscopy in HCI: Toward evaluation methods and adaptive interfaces</strong>， A. Girouard, L.M. Hirshfield, E. Solovey, and R.J.K. Jacob, <em>Proc. ACM CHI 2008 Workshop on Brain-Computer Interfaces for HCI and Games</em> (2008). <a href="http://www.cs.tufts.edu/~jacob/papers/bciworkshop.chi08.pdf" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li></ul> <h3 id="technical-reports"><a href="#technical-reports" class="header-anchor">#</a> Technical Reports</h3> <ul><li><strong>Distinguishing Difficulty Levels with Non-invasive Brain Activity Measurements</strong>, A. Girouard, E.T. Solovey, L.M. Hirshfield, K. Chauncey, A. Sassaroli, S. Fantini, and R.J.K. Jacob, <em>Technical Report</em> 2008-3, Department of Computer Science, Tufts University, Medford, Mass. (2008). <a href="http://www.cs.tufts.edu/tr/techreps/TR-2008-3" target="_blank" rel="noopener noreferrer">link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="acknowledgments"><a href="#acknowledgments" class="header-anchor">#</a> Acknowledgments</h2> <hr> <p>We thank Claire Lee, Matthew Knowles, James Kebinger, Kristof Redei, Kelly Moran, Hadar Rosenhand, and the rest of our colleagues in the HCI research group at Tufts; Dr. Roni Khardon, Dr. Carla Brodley, Dr. Eric Miller, Rachel Lomasky, Umaa Rebbapragada, Amelio Vazquez-Reina, and D. Sculley from the computer science department at Tufts; Krysta Chauncy and Martin Paczynski from the Psychology Department at Tufts; and Desney Tan at Microsoft Research for his helpful inputs and encouragement.</p> <p>We thank the National Science Foundation for support of this research (NSF Grant Nos. IIS-0713506 and IIS-0414389). Any opinions, findings, and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the National Science Foundation. We also thank the Natural Sciences and Engineering Research Council of Canada for financial support.</p> <h2 id="courses"><a href="#courses" class="header-anchor">#</a> Courses</h2> <hr> <p><a href="http://www.cs.tufts.edu/~jacob/250bci/" target="_blank" rel="noopener noreferrer">COMP250 Brain-Computer Interaction<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <div class="content-time" data-v-22a41398><time datetime="Jan 21, 2019" class="time-text" data-v-22a41398>Create Time: Jan 21, 2019
    </time> <time datetime="May 24, 2021" class="time-text" data-v-22a41398>Last Updated: May 24, 2021
    </time></div></article> <section class="flex-xb main info-nav" data-v-64012905 data-v-441751fb><a href="/projects/Taming_fNIRS-based_BCI_Input.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/code_and_datasets/three_phases.png" loading="lazy" alt="Taming fNIRS-based BCI Input for Better Calibration and Broader Use" class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>Taming fNIRS-based BCI Input for Better Calibration and Broader Use</h2> <div class="content" data-v-64012905></div></article></a> <a href="/projects/Tangible_Programming_Languages_A_practical_approach_to_computer_programming_in_educational_settings.html" class="flex-xb nav-item" data-v-64012905><div class="flex-xcc item-img" data-v-64012905><img data-src="/project_images/tern/tangk.jpg" loading="lazy" alt="Tangible Programming Languages A practical approach to computer programming in educational settings." class="img lazy" data-v-64012905></div> <article class="flex-ysc item-content" data-v-64012905><h2 class="content-title" data-v-64012905>Tangible Programming Languages A practical approach to computer programming in educational settings.</h2> <div class="content" data-v-64012905></div></article></a></section> <!----></section></section> <div data-v-55aa431a data-v-4ba9589d><div class="my-box" data-v-55aa431a></div> <footer class="footer" data-v-55aa431a><nav class="link-list" data-v-55aa431a><a href="/projects/Brain-Computer_Interaction_using%20_fNIRS.html" aria-current="page" class="list-item router-link-exact-active router-link-active" data-v-55aa431a>HCI Lab, Department of Computer Science 196 Boston Ave., Tufts University, Medford, MA, 02155</a></nav> <a href="/" class="copyright router-link-active" data-v-55aa431a>Copyright  ©  Tufts University School of Engineering. All Rights Reserved.
      </a></footer></div></section><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.ae6e9df8.js" defer></script><script src="/assets/js/49.ffe0fb15.js" defer></script>
  </body>
</html>
